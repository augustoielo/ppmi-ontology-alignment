{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 01 â€” Parse PDON & PMDO ontologies (Colab)\n\nThis notebook loads **PDON** and **PMDO** from files in your project folder, validates parsing, extracts basic statistics (classes/properties), and optionally exports Turtle versions for easier downstream work.\n\n**Inputs** (place in `ontologies/`):\n- `pdon.xrdf`\n- `pmdo.xrdf`\n\n**Outputs** (written to `output/`):\n- `pdon.ttl` (optional)\n- `pmdo.ttl` (optional)\n- `ontologies_summary.csv`\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Install dependencies"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!pip -q install rdflib pandas owlrl"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Project folders (Google Drive recommended)\n\nThis project is designed to be shareable and resilient across notebooks.\n\n- We mount Google Drive\n- Ensure the project folder exists\n- Ensure standard subfolders exist\n\n> If you prefer not to use Drive, set `USE_DRIVE = False` and it will work in `/content` (ephemeral).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pathlib import Path\nimport os\n\nUSE_DRIVE = True  # set to False to work in /content only\n\nif USE_DRIVE:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    PROJECT_DIR = Path('/content/drive/MyDrive/ppmi-ontology-alignment')\nelse:\n    PROJECT_DIR = Path('/content/ppmi-ontology-alignment')\n\n# Create project folder + standard subfolders\nPROJECT_DIR.mkdir(parents=True, exist_ok=True)\n\nONTO = PROJECT_DIR / 'ontologies'\nMAPS = PROJECT_DIR / 'mapping'\nDATA = PROJECT_DIR / 'data'\nOUT  = PROJECT_DIR / 'output'\n\nfor p in [ONTO, MAPS, DATA, OUT]:\n    p.mkdir(parents=True, exist_ok=True)\n\nprint('PROJECT_DIR:', PROJECT_DIR)\nprint('ONTO:', ONTO)\nprint('MAPS:', MAPS)\nprint('DATA:', DATA)\nprint('OUT :', OUT)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Upload files (if needed)\n\nIf `pdon.xrdf` and `pmdo.xrdf` are already in `ontologies/`, you can skip this step.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from google.colab import files\nimport shutil\n\nrequired = ['pdon.xrdf', 'pmdo.xrdf']\nmissing = [f for f in required if not (ONTO / f).exists()]\nprint('Missing:', missing)\n\nif missing:\n    uploaded = files.upload()\n    for fname in uploaded.keys():\n        src = Path('/content') / fname\n        dst = ONTO / fname\n        shutil.move(str(src), str(dst))\n    print('Uploaded to:', ONTO)\nelse:\n    print('All required ontology files are present in:', ONTO)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Robust parsing with rdflib\n\n`.xrdf` is not a standard extension, so we try common RDF/OWL formats.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from rdflib import Graph\n\ndef load_graph(path: Path):\n    g = Graph()\n    tried = []\n    for fmt in ['xml', 'application/rdf+xml', 'turtle', 'n3', 'nt']:\n        try:\n            g.parse(str(path), format=fmt)\n            return g, fmt, tried\n        except Exception as e:\n            tried.append((fmt, str(e)[:220]))\n            continue\n    # last attempt: autodetect\n    g.parse(str(path))\n    return g, 'auto', tried\n\npdon_path = ONTO / 'pdon.xrdf'\npmdo_path = ONTO / 'pmdo.xrdf'\n\npdon_g, pdon_fmt, pdon_tried = load_graph(pdon_path)\npmdo_g, pmdo_fmt, pmdo_tried = load_graph(pmdo_path)\n\nprint('PDON triples:', len(pdon_g), 'format:', pdon_fmt)\nprint('PMDO triples:', len(pmdo_g), 'format:', pmdo_fmt)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Extract basic ontology statistics\n\nWe count OWL classes and properties, and extract ontology IRI(s) if present.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nfrom rdflib.namespace import RDF, RDFS, OWL\n\ndef ontology_stats(g: Graph, name: str):\n    ont_iris = list(g.subjects(RDF.type, OWL.Ontology))\n    classes  = set(g.subjects(RDF.type, OWL.Class))\n    objp     = set(g.subjects(RDF.type, OWL.ObjectProperty))\n    datap    = set(g.subjects(RDF.type, OWL.DatatypeProperty))\n    annp     = set(g.subjects(RDF.type, OWL.AnnotationProperty))\n    return {\n        'ontology': name,\n        'triples': len(g),\n        'ontology_iris': ';'.join(str(x) for x in ont_iris[:10]),\n        'n_classes': len(classes),\n        'n_object_properties': len(objp),\n        'n_datatype_properties': len(datap),\n        'n_annotation_properties': len(annp),\n    }\n\nstats = pd.DataFrame([\n    ontology_stats(pdon_g, 'PDON'),\n    ontology_stats(pmdo_g, 'PMDO'),\n])\n\nstats\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Save summary to CSV"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "summary_csv = OUT / 'ontologies_summary.csv'\nstats.to_csv(summary_csv, index=False)\nprint('Wrote:', summary_csv)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Quick label search helpers\n\nUseful for locating candidate classes/properties by `rdfs:label`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from rdflib.namespace import RDFS\n\ndef find_by_label(g: Graph, query: str, limit: int = 25):\n    q = query.lower()\n    hits = []\n    for s, _, o in g.triples((None, RDFS.label, None)):\n        if q in str(o).lower():\n            hits.append((str(s), str(o)))\n            if len(hits) >= limit:\n                break\n    return pd.DataFrame(hits, columns=['iri', 'label'])\n\n# Example searches (edit freely)\nfind_by_label(pdon_g, 'parkinson', limit=10)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Optional: export Turtle versions\n\nThis can make later processing easier. Turtle files are written to `output/`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "pdon_ttl = OUT / 'pdon.ttl'\npmdo_ttl = OUT / 'pmdo.ttl'\n\npdon_g.serialize(destination=str(pdon_ttl), format='turtle')\npmdo_g.serialize(destination=str(pmdo_ttl), format='turtle')\n\nprint('Wrote:', pdon_ttl)\nprint('Wrote:', pmdo_ttl)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Next notebook\n\nProceed to `02_build_bridge_ontology.ipynb` to create the **bridge ontology** with:\n- `owl:imports` to PDON and PMDO\n- minimal T-box for Subject/Visit/Observation\n- mapping scaffolding for PPMI variables\n"
    }
  ],
  "metadata": {
    "colab": {
      "name": "01_parse_ontologies.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
