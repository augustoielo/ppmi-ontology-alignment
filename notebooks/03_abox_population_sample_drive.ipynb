{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 03 â€” Populate a sample A-box (Subjects, Visits, Observations)\n\nThis notebook builds an **A-box** from a longitudinal table (PPMI-like), using:\n\n- the bridge ontology: `ontologies/ppmi_bridge.ttl`\n- the mapping file: `mapping/ppmi_pdon_pmdo_mapping.csv`\n\nIt creates:\n- `ppmi:Subject` individuals (by `PATNO`)\n- `ppmi:Visit` individuals (by `PATNO` + `EVENT_ID`)\n- `ppmi:Observation` individuals for mapped variables\n- `ppmi:DiagnosisObservation` for **PRIMDIAG** codes (baseline-only, by default)\n\n**Input data**: you may provide the **full PPMI file** (many columns). The script will read it and only use columns required by the mapping + meta fields.\n\n**Output**: `output/ppmi_abox_sample.ttl`\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!pip -q install rdflib pandas owlrl"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Project root (Drive recommended)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pathlib import Path\nUSE_DRIVE = True\nif USE_DRIVE:\n  from google.colab import drive\n  drive.mount('/content/drive')\n  PROJECT_DIR = Path('/content/drive/MyDrive/ppmi-ontology-alignment')\nelse:\n  PROJECT_DIR = Path('/content/ppmi-ontology-alignment')\n\nONT_DIR = PROJECT_DIR / 'ontologies'\nMAP_DIR = PROJECT_DIR / 'mapping'\nDATA_DIR = PROJECT_DIR / 'data'\nOUT_DIR = PROJECT_DIR / 'output'\nfor p in [ONT_DIR, MAP_DIR, DATA_DIR, OUT_DIR]:\n  p.mkdir(parents=True, exist_ok=True)\n\nBRIDGE_PATH = ONT_DIR / 'ppmi_bridge.ttl'\nMAP_PATH    = MAP_DIR / 'ppmi_pdon_pmdo_mapping.csv'\nDEMO_PATH   = DATA_DIR / 'ppmi_demo.tsv'\nABOX_OUT    = OUT_DIR / 'ppmi_abox_sample.ttl'\n\nprint('PROJECT_DIR:', PROJECT_DIR)\nprint('Bridge exists :', BRIDGE_PATH.exists(), BRIDGE_PATH)\nprint('Mapping exists:', MAP_PATH.exists(), MAP_PATH)\nprint('Demo exists   :', DEMO_PATH.exists(), DEMO_PATH)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Load mapping CSV"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nfrom google.colab import files\nimport shutil\n\nif not MAP_PATH.exists():\n  print('Upload ppmi_pdon_pmdo_mapping.csv')\n  up = files.upload()\n  for fn in up.keys():\n    shutil.move(f'/content/{fn}', str(MAP_PATH if fn=='ppmi_pdon_pmdo_mapping.csv' else (MAP_DIR/fn)))\n\nmapping = pd.read_csv(MAP_PATH)\n\n# Normalise key fields\nfor col in ['Category','Variable','Code','Decode','MappingBucket','TargetIRI','How','Confidence']:\n  if col in mapping.columns:\n    mapping[col] = mapping[col].astype(str)\n\nmapping['Variable'] = mapping['Variable'].astype(str).str.strip()\nmapping['TargetIRI'] = mapping['TargetIRI'].astype(str).str.strip()\nmapping.head(10)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Provide / upload longitudinal data\n\nExpected format: **TSV** with one row per (PATNO, EVENT_ID).\n\nSave as `data/ppmi_demo.tsv` (tab-separated).\n\nYou can also use a larger/full file; the script will only use needed columns.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from google.colab import files\nimport shutil\n\nif not DEMO_PATH.exists():\n  print('Upload ppmi_demo.tsv (tab-separated)')\n  up = files.upload()\n  for fn in up.keys():\n    dest = DEMO_PATH if fn=='ppmi_demo.tsv' else (DATA_DIR/fn)\n    shutil.move(f'/content/{fn}', str(dest))\n  print('Saved to:', DEMO_PATH)\n\n# NOTE: we will load the TSV after computing required columns (usecols) in Step 4.\nprint('TSV path ready:', DEMO_PATH)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Build A-box (RDF)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from rdflib import Graph, Namespace, URIRef, Literal\nfrom rdflib.namespace import RDF, RDFS, OWL, XSD\nimport re\n\nPPMI = Namespace('http://example.org/ppmi-ontology-alignment#')\n\n# -----------------------------\n# Helpers\n# -----------------------------\n\ndef mint_subject(patno: str) -> URIRef:\n  return PPMI[f'subject/{patno}']\n\ndef mint_visit(patno: str, event_id: str) -> URIRef:\n  return PPMI[f'visit/{patno}/{event_id}']\n\ndef mint_obs(patno: str, event_id: str, var: str) -> URIRef:\n  safe = re.sub(r'[^A-Za-z0-9_\\-\\.]+','_', var)\n  return PPMI[f'obs/{patno}/{event_id}/{safe}']\n\ndef parse_gYearMonth(s: str):\n  if s is None:\n    return None\n  m = re.match(r'^(\\d{1,2})/(\\d{4})$', str(s).strip())\n  if not m:\n    return None\n  mm, yy = int(m.group(1)), int(m.group(2))\n  if mm < 1 or mm > 12:\n    return None\n  return f'{yy:04d}-{mm:02d}'\n\ndef _clean(v):\n  if v is None:\n    return ''\n  s = str(v).strip()\n  return '' if s.lower()=='nan' else s\n\ndef literal_best(x):\n  import pandas as pd\n  if x is None:\n    return None\n  if isinstance(x, float) and pd.isna(x):\n    return None\n  s = str(x).strip()\n  if s == '' or s.lower() == 'nan':\n    return None\n\n  # Integers\n  if re.fullmatch(r'[-+]?\\d+', s):\n    try:\n      return Literal(int(s), datatype=XSD.integer)\n    except Exception:\n      pass\n\n  # Decimals / scientific\n  if re.fullmatch(r'[-+]?\\d*\\.\\d+(?:[eE][-+]?\\d+)?', s) or re.fullmatch(r'[-+]?\\d+\\.\\d*', s):\n    try:\n      return Literal(float(s), datatype=XSD.decimal)\n    except Exception:\n      pass\n\n  return Literal(s, datatype=XSD.string)\n\ndef normalize_code_for_lookup(x):\n  \"\"\"Normalise categorical code values to a stable string key.\n  Handles inputs like '1', '1.0', 1, 1.0 -> '1'.\n  Leaves non-numeric strings untouched.\n  \"\"\"\n  if x is None:\n    return ''\n  s = str(x).strip()\n  if s == '' or s.lower() == 'nan':\n    return ''\n  # numeric?\n  if re.fullmatch(r'[-+]?\\d+(?:\\.\\d+)?', s):\n    try:\n      f = float(s)\n      if abs(f - round(f)) < 1e-9:\n        return str(int(round(f)))\n      # keep compact form (no trailing zeros)\n      return ('%s' % f).rstrip('0').rstrip('.')\n    except Exception:\n      return s\n  return s\n\n# -----------------------------\n# Mapping lookups\n# -----------------------------\n\nmap_rows = mapping.copy()\n\n# variable -> (target IRI) for single-target variables\nvar2target = {}\nfor _, r in map_rows.iterrows():\n  var = _clean(r.get('Variable'))\n  tgt = _clean(r.get('TargetIRI'))\n  if var and tgt:\n    # If multiple rows exist for same Variable (e.g., PRIMDIAG), we do NOT overwrite here.\n    if var != 'PRIMDIAG':\n      var2target[var] = tgt\n\n# PRIMDIAG: code -> (decode, targetIRI)\nprim = map_rows[map_rows['Variable'].astype(str).str.strip() == 'PRIMDIAG'].copy()\nprim_lookup = {}\nfor _, r in prim.iterrows():\n  code_key = normalize_code_for_lookup(r.get('Code'))\n  if not code_key:\n    continue\n  dec = _clean(r.get('Decode'))\n  tgt = _clean(r.get('TargetIRI'))\n  prim_lookup[code_key] = (dec, tgt)\n\n# Identify DATSCAN vars and \"observation\" vars\n# Note: we include subject/visit meta columns in the data file but we must not create Observations for them.\n\ndatscan_vars = set(map_rows.loc[map_rows['Category'].astype(str).str.upper().eq('DATSCAN'), 'Variable'].astype(str).str.strip())\n\n# Variables that should become Observations in the A-box\nobs_vars = sorted(set(\n  map_rows.loc[\n    map_rows['MappingBucket'].astype(str).str.upper().isin(['PMDO','MDO','PDON','PPMI/PMDO','OBS_STRUCT']),\n    'Variable'\n  ].astype(str).str.strip()\n))\n\n# Exclude structural/meta buckets from observations (best effort from CSV)\nmeta_buckets = set(['SUBJECT_META','VISIT_META'])\nmeta_vars = set(map_rows.loc[map_rows['MappingBucket'].astype(str).str.upper().isin(meta_buckets), 'Variable'].astype(str).str.strip())\nobs_vars = [v for v in obs_vars if v and v not in meta_vars]\n\n# --- HARD EXCLUSION: never create Observations for these known meta fields ---\nSUBJECT_META = ['COHORT','subgroup','SEX','EDUCYRS','fampd_bin','APOE']\nVISIT_META   = ['EVENT_ID','visit_date','YEAR','age_at_visit']\nobs_vars = [v for v in obs_vars if v not in set(SUBJECT_META + VISIT_META + ['PATNO'])]\n\n# -----------------------------\n# Load longitudinal TSV (usecols) - supports full PPMI TSV safely\n# -----------------------------\n\nrequired_cols = set(['PATNO','EVENT_ID','PRIMDIAG']) | set(SUBJECT_META) | set(VISIT_META) | set(obs_vars)\n\nheader_cols = pd.read_csv(DEMO_PATH, sep='\\t', nrows=0).columns\nusecols = [c for c in header_cols if c in required_cols]\nmissing = sorted(required_cols - set(usecols))\n\nprint('Using cols:', len(usecols))\nif missing:\n  print('Missing cols (ok if not in this file):', missing[:30], '...' if len(missing) > 30 else '')\n\n# Read as strings first to avoid float coercions on categorical codes\n# We'll cast values later with literal_best.\ndemo = pd.read_csv(DEMO_PATH, sep='\\t', dtype=str, usecols=usecols)\nprint('Rows:', len(demo), 'Cols:', len(demo.columns))\n\n# -----------------------------\n# Graph + T-box terms\n# -----------------------------\n\ng = Graph()\ng.bind('ppmi', PPMI)\ng.bind('rdf', RDF)\ng.bind('rdfs', RDFS)\ng.bind('owl', OWL)\ng.bind('xsd', XSD)\n\nC_Subject = PPMI['Subject']\nC_Visit = PPMI['Visit']\nC_Obs = PPMI['Observation']\nC_DxObs = PPMI['DiagnosisObservation']\nC_ImgObs = PPMI['ImagingObservation']\n\nP_hasVisit = PPMI['hasVisit']\nP_hasObs = PPMI['hasObservation']\nP_observes = PPMI['observesConcept']\nP_refersPdon = PPMI['refersToPdonConcept']\nP_relRegion = PPMI['relatesToRegion']\n\nP_hasValue = PPMI['hasValue']\nP_hasCode  = PPMI['hasCode']\nP_hasDecode= PPMI['hasDecode']\nP_visitDate= PPMI['visitDate']\nP_visitYear= PPMI['visitYear']\nP_ageAtVisit=PPMI['ageAtVisit']\n\n\ndef add_subject_meta(subj, row):\n  for v in SUBJECT_META:\n    if v in row.index:\n      lit = literal_best(row[v])\n      if lit is not None:\n        g.add((subj, PPMI[f'subjectMeta/{v}'], lit))\n\ndef add_visit_meta(visit, row):\n  # visit_date: prefer gYearMonth if in MM/YYYY\n  if 'visit_date' in row.index:\n    s = str(row['visit_date']).strip() if _clean(row['visit_date']) else ''\n    gy = parse_gYearMonth(s)\n    if gy:\n      g.add((visit, P_visitDate, Literal(gy, datatype=XSD.gYearMonth)))\n    elif s:\n      g.add((visit, P_visitDate, Literal(s, datatype=XSD.string)))\n\n  if 'YEAR' in row.index:\n    lit = literal_best(row['YEAR'])\n    if lit is not None:\n      g.add((visit, P_visitYear, lit))\n\n  if 'age_at_visit' in row.index:\n    lit = literal_best(row['age_at_visit'])\n    if lit is not None:\n      g.add((visit, P_ageAtVisit, lit))\n\n# -----------------------------\n# Choose scope: one subject sample or full dataset\n# -----------------------------\n\n# By default, create a small A-box for one PATNO (first row) to keep output manageable.\n# If you want the full dataset, set BUILD_ALL = True.\nBUILD_ALL = False\n\n# Optional safety cap when BUILD_ALL=True (set to None to disable)\nMAX_SUBJECTS = None\n\nif 'PATNO' not in demo.columns or 'EVENT_ID' not in demo.columns:\n  raise ValueError('Input TSV must contain at least PATNO and EVENT_ID columns')\n\nif BUILD_ALL:\n  work_df = demo.copy()\n  if MAX_SUBJECTS:\n    pats = work_df['PATNO'].dropna().astype(str).str.strip().unique().tolist()\n    keep = set(pats[:MAX_SUBJECTS])\n    work_df = work_df[work_df['PATNO'].astype(str).str.strip().isin(keep)].copy()\n    print('BUILD_ALL=True with MAX_SUBJECTS ->', MAX_SUBJECTS, 'subjects, rows:', len(work_df))\n  else:\n    print('BUILD_ALL=True -> using all rows:', len(work_df))\nelse:\n  SAMPLE_PATNO = str(demo['PATNO'].iloc[0]).strip()\n  work_df = demo[demo['PATNO'].astype(str).str.strip() == SAMPLE_PATNO].copy()\n  print('BUILD_ALL=False -> Sample PATNO:', SAMPLE_PATNO, 'rows:', len(work_df))\n\n# Option: PRIMDIAG baseline-only\nPRIMDIAG_BASELINE_ONLY = True\n\n# -----------------------------\n# Populate\n# -----------------------------\n\nfor _, row in work_df.iterrows():\n  patno = str(row['PATNO']).strip()\n  event = str(row['EVENT_ID']).strip()\n  if patno == '' or event == '':\n    continue\n\n  subj = mint_subject(patno)\n  visit = mint_visit(patno, event)\n\n  g.add((subj, RDF.type, C_Subject))\n  g.add((visit, RDF.type, C_Visit))\n  g.add((subj, P_hasVisit, visit))\n\n  add_subject_meta(subj, row)\n  add_visit_meta(visit, row)\n\n  # Create observations for mapped variables\n  for var in obs_vars:\n    if var not in row.index:\n      continue\n\n    # Guard: meta fields must never become Observations\n    if var in SUBJECT_META or var in VISIT_META or var in ('PATNO', 'EVENT_ID'):\n      continue\n\n    # Handle PRIMDIAG separately\n    if var == 'PRIMDIAG':\n      if PRIMDIAG_BASELINE_ONLY and event != 'BL':\n        continue\n\n      raw = row.get('PRIMDIAG', None)\n      code_key = normalize_code_for_lookup(raw)\n      if not code_key:\n        continue\n\n      obs = mint_obs(patno, event, var)\n      g.add((visit, P_hasObs, obs))\n\n      g.add((obs, RDF.type, C_DxObs))\n      g.add((obs, P_hasCode, Literal(code_key, datatype=XSD.string)))\n\n      dec, tgt = prim_lookup.get(code_key, ('',''))\n      if dec:\n        g.add((obs, P_hasDecode, Literal(dec, datatype=XSD.string)))\n      if tgt:\n        g.add((obs, P_refersPdon, URIRef(tgt)))\n\n      continue\n\n    # Normal observation variables\n    val_lit = literal_best(row[var])\n    if val_lit is None:\n      continue\n\n    obs = mint_obs(patno, event, var)\n    g.add((visit, P_hasObs, obs))\n\n    # DATSCAN region variables (MIA_*) -> ImagingObservation + relatesToRegion\n    if var in datscan_vars and var.startswith('MIA_'):\n      g.add((obs, RDF.type, C_ImgObs))\n      tgt_region = var2target.get(var)\n      if tgt_region:\n        g.add((obs, P_relRegion, URIRef(tgt_region)))\n    else:\n      g.add((obs, RDF.type, C_Obs))\n\n    g.add((obs, P_hasValue, val_lit))\n\n    tgt = var2target.get(var)\n    if tgt:\n      g.add((obs, P_observes, URIRef(tgt)))\n\nprint('A-box triples:', len(g))\nprint('PRIMDIAG codes in mapping lookup:', len(prim_lookup))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Serialise A-box to Turtle"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "g.serialize(destination=str(ABOX_OUT), format='turtle')\nprint('Wrote:', ABOX_OUT)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Quick checks (SPARQL)\n\n- A richer sample listing (value/code/decode/concepts)\n- Sanity: PRIMDIAG mapped vs unmapped\n- Sanity: PRIMDIAG baseline-only violations\n- Sanity: APOE should NOT appear as Observation\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "q = '''\nPREFIX ppmi: <http://example.org/ppmi-ontology-alignment#>\nSELECT ?obs ?type ?value ?code ?decode ?concept ?pdon\nWHERE {\n  ?visit ppmi:hasObservation ?obs .\n  OPTIONAL { ?obs a ?type . }\n  OPTIONAL { ?obs ppmi:hasValue ?value }\n  OPTIONAL { ?obs ppmi:hasCode ?code }\n  OPTIONAL { ?obs ppmi:hasDecode ?decode }\n  OPTIONAL { ?obs ppmi:observesConcept ?concept }\n  OPTIONAL { ?obs ppmi:refersToPdonConcept ?pdon }\n}\nLIMIT 50\n'''\nfor row in g.query(q):\n  print(row)\n\nprint('\\n--- Counts ---')\nq_counts = '''\nPREFIX ppmi: <http://example.org/ppmi-ontology-alignment#>\nSELECT\n  (COUNT(DISTINCT ?s) AS ?nSubjects)\n  (COUNT(DISTINCT ?v) AS ?nVisits)\n  (COUNT(DISTINCT ?o) AS ?nObs)\n  (COUNT(DISTINCT ?dx) AS ?nDxObs)\nWHERE {\n  OPTIONAL { ?s a ppmi:Subject . }\n  OPTIONAL { ?v a ppmi:Visit . }\n  OPTIONAL { ?o a ppmi:Observation . }\n  OPTIONAL { ?dx a ppmi:DiagnosisObservation . }\n}\n'''\nprint(list(g.query(q_counts))[0])\n\nprint('\\n--- PRIMDIAG: mapped vs unmapped ---')\nq_dx_map = '''\nPREFIX ppmi: <http://example.org/ppmi-ontology-alignment#>\nSELECT\n  (COUNT(DISTINCT ?dx) AS ?nDxTotal)\n  (COUNT(DISTINCT ?dxWithPdon) AS ?nDxWithPdon)\n  (COUNT(DISTINCT ?dxNoPdon) AS ?nDxNoPdon)\nWHERE {\n  ?dx a ppmi:DiagnosisObservation ;\n      ppmi:hasCode ?code .\n  OPTIONAL { ?dx ppmi:refersToPdonConcept ?pdon . }\n  BIND( IF(BOUND(?pdon), ?dx, UNDEF) AS ?dxWithPdon )\n  BIND( IF(!BOUND(?pdon), ?dx, UNDEF) AS ?dxNoPdon )\n}\n'''\nprint(list(g.query(q_dx_map))[0])\n\nprint('\\n--- PRIMDIAG baseline-only violations (should be 0 rows) ---')\nq_dx_not_bl = '''\nPREFIX ppmi: <http://example.org/ppmi-ontology-alignment#>\nSELECT ?visit ?dx\nWHERE {\n  ?visit ppmi:hasObservation ?dx .\n  ?dx a ppmi:DiagnosisObservation .\n  FILTER(!CONTAINS(STR(?visit), \"/BL\"))\n}\nLIMIT 20\n'''\nrows = list(g.query(q_dx_not_bl))\nprint('Dx not BL:', len(rows))\nfor r in rows:\n  print(r)\n\nprint('\\n--- APOE must NOT be an Observation (should be 0 rows) ---')\nq_apoe_obs = '''\nPREFIX ppmi: <http://example.org/ppmi-ontology-alignment#>\nSELECT ?visit ?obs\nWHERE {\n  ?visit ppmi:hasObservation ?obs .\n  FILTER(CONTAINS(STR(?obs), \"/APOE\"))\n}\nLIMIT 20\n'''\nrows = list(g.query(q_apoe_obs))\nprint('APOE as Observation:', len(rows))\nfor r in rows:\n  print(r)\n"
    }
  ],
  "metadata": {
    "colab": {
      "name": "03_abox_population_sample_drive.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
